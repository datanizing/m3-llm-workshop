{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402528d4-dc4f-4334-8682-dc908103486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "[2025-03-19 15:47:42,842] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abb8ab8-4cfc-441e-a5ce-db8f9237e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "dataset_name = 'ai-abstract-dataset.jsonl.xz'\n",
    "output_dir = \"runs/\" + model_name.split(\"/\")[-1] + dataset_name.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9839012d-6041-4111-a099-3dc36bd52841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.524 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12a5b595f9144ba8d05db51228de94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(model_name, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None, # autodetect\n",
    "    load_in_4bit = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78cd128b-a4bb-4609-b01a-7e6cacf42b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c318f0-ae68-4fba-bdb3-ad8ab579e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d48b411-0bf0-47bf-8093-4a5826cdd2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2eeb49f-d33b-41ec-ac7a-a6838bfc0205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e774d910-7fa3-4d57-bf90-5bce1c85218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{ \"content\": \"\"\"You are an educated researcher and always answer in correct scientific terms.\n",
    "                             You are very deep into AI and its methodologies. You are very creative.\"\"\", \n",
    "              \"role\": \"system\" }, \n",
    "            { \"content\": \"Write an abstract with the title 'New Training Methods for LLMs'\", \n",
    "              \"role\": \"user\" },\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a52ae0-1264-4191-be5d-eefaf097adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e71c4ec-3d30-492b-a5c7-9056eefc5a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are an educated researcher and always answer in correct scientific terms.\n",
      "                             You are very deep into AI and its methodologies. You are very creative.<|end|><|user|> Write an abstract with the title 'Frontiers in Increasing Random Forest Classification'<|end|><|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d302afca-5f20-4aa2-b5d5-ffd3cb8ad544",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens = 512, use_cache = True,\n",
    "                         do_sample=True, temperature=0.7, top_k=25, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44a0aca1-b181-4432-bddd-0ee22b766d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are an educated researcher and always answer in correct scientific terms.\n",
      "                             You are very deep into AI and its methodologies. You are very creative.<|end|><|user|> Write an abstract with the title 'Frontiers in Increasing Random Forest Classification'<|end|><|assistant|> This paper presents a comprehensive exploration of the frontiers in increasing Random Forest classification. The Random Forest algorithm, a popular ensemble learning method, has demonstrated remarkable performance in various machine learning tasks. However, the quest for improved accuracy, efficiency, and generalization continues to drive research in this area. This paper provides an in-depth analysis of recent advancements in Random Forest classification, including novel techniques for feature selection, ensemble learning strategies, and integration with deep learning architectures. We discuss the challenges and opportunities in optimizing Random Forest algorithms for different applications, such as image classification, natural language processing, and time series analysis. Additionally, we explore the potential of combining Random Forest with other machine learning methods, such as neural networks and support vector machines, to enhance performance and tackle complex classification tasks. This paper aims to provide a comprehensive overview of the current state-of-the-art in Random Forest classification, highlighting its strengths, limitations, and future directions. By exploring the frontiers in increasing Random Forest classification, we aim to inspire researchers and practitioners to develop innovative solutions that push the boundaries of machine learning and artificial intelligence.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e65d6d8-254e-4c7e-a2f7-96acc6d7dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens = 512, use_cache = True,\n",
    "                         do_sample=True, temperature=0.7, top_k=25, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c536042-2dd0-4546-b716-d11ade885e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are an educated researcher and always answer in correct scientific terms.\n",
      "                             You are very deep into AI and its methodologies. You are very creative.<|end|><|user|> Write an abstract with the title 'New Training Methods for LLMs'<|end|><|assistant|> The training of Large Language Models (LLMs) is a complex and time-consuming process that involves optimizing a large number of parameters using gradient descent. In this paper, we propose new training methods that can improve the efficiency and effectiveness of LLM training. Our methods include techniques such as knowledge distillation, parameter-efficient training, and data augmentation. We demonstrate the effectiveness of our methods through experiments on LLMs such as GPT-3 and BERT. Our results show that our methods can improve the performance of LLMs while reducing the training time and computational resources required.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f26b2ba5-5409-4cfe-a884-3b8681cc3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{ \"content\": \"\"\"You are an educated researcher and always answer in correct scientific terms.\n",
    "                             You are very deep into AI and its methodologies. You are very creative.\"\"\", \n",
    "              \"role\": \"system\" }, \n",
    "            { \"content\": \"Write an abstract with the title 'Frontiers in Increasing Random Forest Classification'\", \n",
    "              \"role\": \"user\" },\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dcf573-7a9e-4ba8-8cdb-3ab0159582c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
