{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8299622c-337f-4df9-a061-b4620cd65938",
   "metadata": {},
   "source": [
    "ideas and script from https://www.philschmid.de/fine-tune-llms-in-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dde0d-4501-4708-9d00-7ce258349e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, ModelConfig, SFTConfig, get_peft_config\n",
    "from datasets import load_dataset\n",
    "# much faster kernel\n",
    "from liger_kernel.transformers import AutoLigerKernelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fd4c2-e302-4fed-9027-1ff4b2790f13",
   "metadata": {},
   "source": [
    "# Arguments and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96929804-9ddb-4a58-b399-1c593aaba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "dataset_name = 'ai-abstract-dataset.jsonl.xz'\n",
    "output_dir = \"runs/\" + model_name.split(\"/\")[-1] + dataset_name.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac60180-1909-4ef8-922d-a0e64b96fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelConfig(model_name_or_path=model_name, \n",
    "                         model_revision='main', \n",
    "                         torch_dtype='bfloat16', \n",
    "                         trust_remote_code=False, \n",
    "                         attn_implementation='flash_attention_2', \n",
    "                         use_peft=True, \n",
    "                         lora_r=16, \n",
    "                         lora_alpha=16, \n",
    "                         lora_dropout=0.05, \n",
    "                         lora_target_modules='all-linear', \n",
    "                         lora_modules_to_save=['lm_head', 'embed_tokens'],\n",
    "                         lora_task_type='CAUSAL_LM', \n",
    "                         use_rslora=False, \n",
    "                         load_in_8bit=False, \n",
    "                         load_in_4bit=True, \n",
    "                         bnb_4bit_quant_type='nf4', \n",
    "                         use_bnb_nested_quant=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405143e-5f82-4e74-9933-529788082a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "     output_dir=output_dir,    \n",
    "     num_train_epochs=1,\n",
    "     bf16=True,\n",
    "     packing=True,\n",
    "     max_length=1024,\n",
    "     per_device_train_batch_size=8,\n",
    "     gradient_accumulation_steps=2,\n",
    "     gradient_checkpointing=True,\n",
    "     gradient_checkpointing_kwargs = { \"use_reentrant\": False },\n",
    "     learning_rate=2.0e-4,\n",
    "     lr_scheduler_type=\"constant\",\n",
    "     use_liger_kernel=True,\n",
    "     warmup_ratio=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73942c34-4e2a-4d46-8dce-a295d0e34389",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83398831-7cd5-4fac-8aef-312d1954ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"json\", data_files=dataset_name, split=\"train\")\n",
    "\n",
    "f'Dataset with {len(train_dataset)} samples and the following features: {train_dataset.features}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32877220-7682-4ce2-81af-5db5a6cef9f6",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292dcff7-47b4-4674-80ba-df57ce94743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None: \n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9da573-6004-4f17-b9b1-428899627050",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811aec60-1f3a-4e0f-aed6-95a9387644d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model kwargs\n",
    "model_kwargs = dict(\n",
    "    revision=model_args.model_revision, # What revision from Huggingface to use, defaults to main\n",
    "    trust_remote_code=model_args.trust_remote_code, # Whether to trust the remote code, this also you to fine-tune custom architectures\n",
    "    attn_implementation=model_args.attn_implementation, # What attention implementation to use, defaults to flash_attention_2\n",
    "    dtype=model_args.torch_dtype, # What torch dtype to use, defaults to auto\n",
    "    use_cache=False if training_args.gradient_checkpointing else True, # Whether\n",
    "    low_cpu_mem_usage=True,  # Reduces memory usage on CPU for loading the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351c0e-2273-4bc6-99a3-3f23d95031f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=model_kwargs['dtype'],\n",
    "    bnb_4bit_quant_storage=model_kwargs['dtype'],\n",
    ")\n",
    "\n",
    "peft_config = get_peft_config(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4b5c6-c6c2-4b9c-85fc-1f7c4523a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoLigerKernelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a00b76-38ee-409b-b0c3-5429d093285e",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e11222-9e27-4fdc-8ff9-2e74b61b3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcd2bf-5285-461d-ade4-4329f3aa7e5b",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a4c33-507e-4b69-8117-77f62579a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "# log metrics\n",
    "metrics = train_result.metrics\n",
    "metrics['train_samples'] = len(train_dataset)\n",
    "trainer.save_metrics('train', metrics)\n",
    "trainer.save_state()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b2438-af85-4869-979d-431a98f712ab",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6e15c-a5c0-4370-8edd-be739bf1981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore k,v cache for fast inference\n",
    "trainer.model.config.use_cache = True\n",
    "trainer.save_model(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "f\"saved to {training_args.output_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7418cf-bb5e-4761-825e-d0793bac96d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
